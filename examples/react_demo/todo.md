



1. Create code to compare implementations
2. For each task run at least once and compare implementations
    2.1 Check for issues in the robustness of the grader
    2.2 Check if any task is badly define or there is any wrong baseline
3. Create a robust way of running the whole eval starting from a json file+docker+data




